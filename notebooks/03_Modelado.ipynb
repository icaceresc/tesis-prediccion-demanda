{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1383036",
   "metadata": {},
   "source": [
    "## Notebook 3: Modelado y Evaluación\n",
    "\n",
    "**Objetivo:** Cargar los subconjuntos de datos para ambas vertientes (unidades y guaraníes), entrenar los modelos (Lineal, Cuadrático, KNN), evaluar su rendimiento y guardar todos los resultados.\n",
    "\n",
    "**Fases:**\n",
    "1.  **Configuración:** Importar librerías y funciones.\n",
    "2.  **Bucle de Ejecución:** Iterar sobre 'cantidad' y 'monto_venta'.\n",
    "    a. Cargar los datasets correspondientes.\n",
    "    b. Ejecutar los experimentos.\n",
    "    c. Guardar los resultados en subcarpetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50caceb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T17:41:03.096837Z",
     "iopub.status.busy": "2025-10-24T17:41:03.096837Z",
     "iopub.status.idle": "2025-10-24T17:41:03.241609Z",
     "shell.execute_reply": "2025-10-24T17:41:03.241008Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "070afdfa",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- Rutas ---\n",
    "BASE_DIR = '..'\n",
    "PROCESSED_DATA_DIR = os.path.join(BASE_DIR, '02_Data', '02_processed')\n",
    "RAW_DATA_DIR = os.path.join(BASE_DIR, '02_Data', '01_raw')\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, '04_Resultados')\n",
    "TABLES_DIR = os.path.join(RESULTS_DIR, '01_Tablas_Detalladas')\n",
    "\n",
    "# --- Parámetros de Ejecución ---\n",
    "target_variable = 'all' # 'cantidad', 'monto_venta' o 'all'\n",
    "datasets_to_run = ['F_48_NP'] # Default actualizado\n",
    "models_to_run = ['Lineal', 'Cuadrático', 'KNN', 'SARIMA']\n",
    "metrics_to_calculate = ['R2', 'MAE', 'RMSE', 'MASE', 'ME']\n",
    "\n",
    "# --- ¡NUEVO PARÁMETRO! ---\n",
    "maxiter = 50 # Valor por defecto para auto_arima (Papermill lo sobrescribirá)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e79fdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T17:41:03.244603Z",
     "iopub.status.busy": "2025-10-24T17:41:03.243681Z",
     "iopub.status.idle": "2025-10-24T17:41:05.604486Z",
     "shell.execute_reply": "2025-10-24T17:41:05.604486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerías y módulos importados. Rutas definidas.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import logging \n",
    "\n",
    "# Importar las funciones del 'motor' (utils.py)\n",
    "try:\n",
    "    from utils.utils import ejecutar_experimentos, cargar_datos_dbf\n",
    "except ImportError:\n",
    "    print(\"Error: No se pudieron importar las funciones desde 'utils.utils'.\")\n",
    "    print(\"Asegúrate de que 03_Code/utils/utils.py existe.\")\n",
    "\n",
    "os.makedirs(TABLES_DIR, exist_ok=True)\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)\n",
    "\n",
    "print(\"Librerías y módulos importados. Rutas definidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f266f32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T17:41:05.607504Z",
     "iopub.status.busy": "2025-10-24T17:41:05.607504Z",
     "iopub.status.idle": "2025-10-24T17:46:12.495195Z",
     "shell.execute_reply": "2025-10-24T17:46:12.495195Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      9\u001b[39m     targets_a_predecir = [target_variable] \n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Cargar información de productos una sola vez\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     _, _, df_info_productos = \u001b[43mcargar_datos_dbf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRAW_DATA_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     df_info_productos = df_info_productos[[\u001b[33m'\u001b[39m\u001b[33mCODI\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDES\u001b[39m\u001b[33m'\u001b[39m]].rename(columns={\u001b[33m'\u001b[39m\u001b[33mCODI\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mcodigo_producto\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDES\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mdescripcion_producto\u001b[39m\u001b[33m'\u001b[39m})\n\u001b[32m     15\u001b[39m     df_info_productos[\u001b[33m'\u001b[39m\u001b[33mcodigo_producto\u001b[39m\u001b[33m'\u001b[39m] = df_info_productos[\u001b[33m'\u001b[39m\u001b[33mcodigo_producto\u001b[39m\u001b[33m'\u001b[39m].str.strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\isaia\\OneDrive\\UPA\\Ing_Industrial\\05 Investigación\\01 Tesis\\03_Code\\utils\\utils.py:31\u001b[39m, in \u001b[36mcargar_datos_dbf\u001b[39m\u001b[34m(ruta_directorio_raw)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# (Tu código... sin cambios)\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     tabla_venta = \u001b[43mDBF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruta_directorio_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mVENTA.DBF\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlatin-1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     venta_df = pd.DataFrame(\u001b[38;5;28miter\u001b[39m(tabla_venta))\n\u001b[32m     33\u001b[39m     tabla_dventa = DBF(os.path.join(ruta_directorio_raw, \u001b[33m'\u001b[39m\u001b[33mDVENTA.DBF\u001b[39m\u001b[33m'\u001b[39m), encoding=\u001b[33m'\u001b[39m\u001b[33mlatin-1\u001b[39m\u001b[33m'\u001b[39m, load=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\isaia\\OneDrive\\UPA\\Ing_Industrial\\05 Investigación\\01 Tesis\\venv\\Lib\\site-packages\\dbfread\\dbf.py:136\u001b[39m, in \u001b[36mDBF.__init__\u001b[39m\u001b[34m(self, filename, encoding, ignorecase, lowernames, parserclass, recfactory, load, raw, ignore_missing_memofile, char_decode_errors)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28mself\u001b[39m.memofilename = \u001b[38;5;28mself\u001b[39m._get_memofilename()\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\isaia\\OneDrive\\UPA\\Ing_Industrial\\05 Investigación\\01 Tesis\\venv\\Lib\\site-packages\\dbfread\\dbf.py:172\u001b[39m, in \u001b[36mDBF.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load records into memory.\u001b[39;00m\n\u001b[32m    166\u001b[39m \n\u001b[32m    167\u001b[39m \u001b[33;03mThis loads both records and deleted records. The ``records``\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[33;03mand ``deleted`` attributes will now be lists of records.\u001b[39;00m\n\u001b[32m    169\u001b[39m \n\u001b[32m    170\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loaded:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     \u001b[38;5;28mself\u001b[39m._records = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_records\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m     \u001b[38;5;28mself\u001b[39m._deleted = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m._iter_records(\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[33m*\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\isaia\\OneDrive\\UPA\\Ing_Industrial\\05 Investigación\\01 Tesis\\venv\\Lib\\site-packages\\dbfread\\dbf.py:315\u001b[39m, in \u001b[36mDBF._iter_records\u001b[39m\u001b[34m(self, record_type)\u001b[39m\n\u001b[32m    311\u001b[39m         items = [(field.name, read(field.length)) \\\n\u001b[32m    312\u001b[39m                  \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fields]\n\u001b[32m    313\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    314\u001b[39m         items = [(field.name,\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m                   parse(field, \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m)) \\\n\u001b[32m    316\u001b[39m                  \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fields]\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recfactory(items)\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m sep \u001b[38;5;129;01min\u001b[39;00m (\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\x1a\u001b[39;00m\u001b[33m'\u001b[39m, \u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    321\u001b[39m     \u001b[38;5;66;03m# End of records.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ### 2. Bucle de Ejecución para Ambas Vertientes\n",
    "\n",
    "# Determinar los targets a procesar basado en el parámetro\n",
    "if isinstance(target_variable, list):\n",
    "    targets_a_predecir = target_variable\n",
    "elif target_variable == 'all':\n",
    "    targets_a_predecir = ['cantidad', 'monto_venta']\n",
    "else:\n",
    "    targets_a_predecir = [target_variable] \n",
    "\n",
    "try:\n",
    "    # Cargar información de productos una sola vez\n",
    "    _, _, df_info_productos = cargar_datos_dbf(RAW_DATA_DIR)\n",
    "    df_info_productos = df_info_productos[['CODI', 'DES']].rename(columns={'CODI': 'codigo_producto', 'DES': 'descripcion_producto'})\n",
    "    df_info_productos['codigo_producto'] = df_info_productos['codigo_producto'].str.strip()\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar 'df_info_productos' de DBF: {e}\")\n",
    "    df_info_productos = pd.DataFrame(columns=['codigo_producto', 'descripcion_producto']) \n",
    "\n",
    "\n",
    "for target in targets_a_predecir:\n",
    "    print(f\"\\n{'='*20} INICIANDO MODELADO (CON CV) PARA: {target.upper()} {'='*20}\")\n",
    "    \n",
    "    # Definir carpetas de entrada y salida\n",
    "    input_subdir = os.path.join(PROCESSED_DATA_DIR, target)\n",
    "    output_subdir = os.path.join(TABLES_DIR, target)\n",
    "    os.makedirs(output_subdir, exist_ok=True)\n",
    "    \n",
    "    # --- Carga de Datos --- (Usando los datasets del parámetro)\n",
    "    print(f\"\\nCargando subconjuntos de datos para modelar '{target}'...\")\n",
    "    nombres_datasets = datasets_to_run\n",
    "    datasets_para_modelar = {}\n",
    "\n",
    "    for nombre in nombres_datasets:\n",
    "        try:\n",
    "            path = os.path.join(input_subdir, f'ventas_{nombre}.csv')\n",
    "            datasets_para_modelar[nombre] = pd.read_csv(path, parse_dates=['mes'], dtype={'codigo_producto': str})\n",
    "            print(f\" - Dataset '{nombre}' cargado. (Dimensiones: {datasets_para_modelar[nombre].shape})\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\" - ADVERTENCIA: No se encontró el archivo para '{nombre}' en {path}. Se omitirá.\")\n",
    "\n",
    "    if not datasets_para_modelar:\n",
    "        print(f\"ADVERTENCIA: No hay datasets cargados para '{target}'. Saltando modelado.\")\n",
    "        continue\n",
    "        \n",
    "    # --- Ejecución de Experimentos (¡Llamada actualizada!) ---\n",
    "    # Ahora pasamos 'maxiter' a la función\n",
    "    logging.info(\"Iniciando la ejecución de experimentos de modelado (con CV)...\")\n",
    "    resultados_completos = ejecutar_experimentos(\n",
    "        datasets_dict=datasets_para_modelar,\n",
    "        df_info_productos=df_info_productos,\n",
    "        target_column=target,\n",
    "        models_to_run=models_to_run, \n",
    "        metrics_to_calculate=metrics_to_calculate,\n",
    "        maxiter_sarima=maxiter # <-- ¡NUEVO PARÁMETRO PASADO!\n",
    "    )\n",
    "\n",
    "    # --- Guardado de Resultados (¡Ordenamiento mejorado!) ---\n",
    "    print(f\"\\nGuardando resultados detallados (CV) para '{target}'...\")\n",
    "    if resultados_completos:\n",
    "        for nombre, df_res in resultados_completos.items():\n",
    "            if df_res.empty:\n",
    "                print(f\" - Sin resultados para '{nombre}'.\")\n",
    "                continue\n",
    "                \n",
    "            path = os.path.join(output_subdir, f'resultados_detallados_{nombre}_{target}_CV.csv') # Añadido _CV al nombre\n",
    "            \n",
    "            # --- ¡LÓGICA DE ORDENAMIENTO MEJORADA! ---\n",
    "            # Ordenar por MASE (ascendente) como métrica principal para la tesis\n",
    "            sort_col = 'MASE_ganador' # Esta columna ahora es el MASE_CV_mean del ganador\n",
    "            ascending_sort = True\n",
    "            \n",
    "            if sort_col not in df_res.columns:\n",
    "                # Fallback si MASE no se calculó\n",
    "                sort_col = 'R2_ganador'\n",
    "                ascending_sort = False\n",
    "            \n",
    "            try:\n",
    "                if sort_col in df_res.columns:\n",
    "                     df_res.sort_values(by=sort_col, ascending=ascending_sort).to_csv(path, index=False, decimal='.', sep=',')\n",
    "                else:\n",
    "                     df_res.to_csv(path, index=False, decimal='.', sep=',') # Guardar sin ordenar si no hay métricas\n",
    "                print(f\" -> Resultados para '{nombre}' guardados en: {path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR al guardar {path}: {e}\")\n",
    "    else:\n",
    "        print(\"\\nNo se generaron resultados. Verifica los pasos anteriores.\") \n",
    "\n",
    "print(f\"\\n{'='*20} PIPELINE DE MODELADO (CON CV) COMPLETADO {'='*20}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
